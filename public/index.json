[{"content":"I am reviewing some fundamental algebra, and I just learnt something beautifully suggesting the connection of ring theory and geometry, which made me eager to learn some algebraic geometry (which I was not very interested when I was in undergrad). It is an exercise in Aluffi\u0026rsquo;s book1.\nLets\u0026rsquo; jump into it.\nThe problem Let $K$ be a compact topological space, and let $R$ be the ring of continuous real-valued functions on $K$, with addition and multiplication defined pointwise. (i) For $p \\in K$, let $M_p = \\{f \\in R | f(p) = 0\\}$. Prove that $M_p$ is a maximal ideal in $R$. (ii) Prove that if $f_1, \\dots , f_r \\in R$ have no common zeros, then $(f1, \\dots , fr)$ = $(1)$. (Hint: Consider $f^2_1 + \\dots + f^2_r$ .) (iii) Prove that every maximal ideal $M$ in $R$ is of the form $M_p$ for some $p \\in K$. (Hint: You will use the compactness of $K$ and (ii).)\nConclude that $p \\mapsto M_p$ defines a bijection from $K$ to the set of maximal ideals of $R$.\n\u0026hellip;and for a complete picture, here is a related problem 2 (also in the same book) to show that compactness of $K$ is necessary:\nLet $R$ be the ring of continuous from $\\mathbb{R}$ to $\\mathbb{R}$. Show that there is a maximal ideal of $R$ such that it is not in the form of $M_p$.\nThought process (in a separate section so that you could skip if you are not interested) (i) is trivial by constructing a ring homomorphism $\\phi$ from $R$ to $\\mathbb{R}$ such that $\\phi (f) = f(p)$.\n(ii) is also straight forward: First notice that $1$ in $R$ is the function $Id$ on $K$ such that $Id(x) = 1 \\forall x \\in K$. So for any $f \\in R$ if $f$ has no zeros it will have an inverse ($\\dfrac{1}{f}$). So what happens if $f_1, \\dots , f_r \\in R$ have no common zeros? They can \u0026ldquo;cover\u0026rdquo; each other. Though it is not simply $f_1 + \\dots + f_r$ since the \u0026ldquo;positive plus negative\u0026rdquo; cases will cause pathologies. But $f^2_1 + \\dots + f^2_r$ obviously solve this problem: it is always positive. And hence has an inverse. Since the ideal $(f_1, \\dots , f_r)$ contains $f_1, \\dots , f_r$, it must contains $f^2_1 + \\dots + f^2_r$ and hence the ideal equals to the whole $R$.\n(iii) is a little bit tricky, but is the beautiful part of the problem. Below is a rough recap of my thought process:\nFirst, from (ii) one should expect that $M$ cannot contains functions with no common zeros, otherwise $M = R$. So maybe let denote $Z$ as the set of common zeros of all the functions in $M$? What does this $Z$ should look like? It is natural to expect that it is non-empty. We want it to be a singleton because that will match the definition of $M_p$. We hope $M = \\{f \\in R | f(Z) = 0\\}$ and $Z$ is a singleton, that seems natural and it will prove the statement straight away.\nBut\u0026hellip;what if it is not? What will happen if there are more than one common zeros\u0026hellip;? Oh actually it does not matter, since obviously $\\{f \\in R | f(Z) = 0\\} \\subseteq \\{f \\in R | f(p) = 0\\}$ for any $p \\in Z$, and since $M$ has to be maximal so as long as we can show $M \\subseteq \\{f \\in R | f(Z) = 0\\}$ it immediately implies that $M \\subseteq \\{f \\in R | f(p) = 0\\}$ for some $p \\in Z$.\nOk, is it even possible for $M$ to be not contained in $\\{f \\in R | f(Z) = 0\\}$\u0026hellip;? It seems impossible, cuz the functions in $M$ must have common zeros or $M$ will eqaul to $R$. Hence any function $f \\in M$ must also be in $\\{f \\in R | f(Z) = 0\\}$ right\u0026hellip;? Huh? Isn\u0026rsquo;t that complete the proof? But I didn\u0026rsquo;t use the compactness of $K$. I should expect it fails when $K$ is not compact, so let\u0026rsquo;s think about a counterexample\u0026hellip;\n[After trying to construct a few functions from $\\mathbb{R}$ to $\\mathbb{R}$ and let them generate an ideal so that the resulting ideal is not contained in $\\{f \\in R | f(p) = 0\\}$ for some $p \\in \\mathbb{R}$, but with no luck, I turn into the following thoughts\u0026hellip;]\nIt seems hard to imagine the statement is not true for general $K$, I must have done something wrong\u0026hellip; Let\u0026rsquo;s look at (ii) again since it causes all the pathologies. Oh wait, it says $f_1, \\dots , f_r \\in R$ should have common zeros, there are only finitely many of them! So in order to fail one must construct a collection of continuous functions such that any finite subcollection of them have common zeros, but the whole collection of them have no common zeros. Looking at the word \u0026ldquo;compact\u0026rdquo;, what can I think of? Finite intersection property!\nand the rest of proving (iii) was straight forward by considering the collection of closed set $f^{-1}_{\\alpha}(0) \\text{ where } M = \\bigcup_{\\alpha} f_{\\alpha}$\nNow I can come up with a counterexample for when $K$ is not compact, consider the the set $C = \\bigcup_{i} \\{f : \\; f(x) = 0 \\; \\forall x \\in (-\\infty , i)\\}$ for $i = -1, -2, -3, \\dots$. It is obviously an ideal, it satisfies the condition that any finite subcollection $f_1, \\dots , f_r$ have common zeros, and yet they are not contained in $M_p = \\{f \\in R | f(p) = 0\\}$ for any $p$.\nAfterthoughts What we have shown is that a compact topological space can be viewed as a collection of maximal ideals of a ring (namely the ring of continuous functions to ANY field). (The problem used $\\mathbb{R}$, but it should work for any field with a topology, since (ii) requires a field to construct the inverse but there is nothing specific to $\\mathbb{R}$).\nSo that makes an interesting connection between the structure of the maximal ideals of a ring and the topology of a compact space.\nWhat information about other topological properties can we deduce from studying those maximal ideals, or vice versa?\nThese are probably questions that people already answered, but I came up with some quick thoughts here:\nwhat if we take $K = \\{0, 1\\} \\subset \\mathbb{R}$? Obviously any function from $K$ to \\mathbb{R} is continuous, so the ring of continuous function $C(\\mathbb{R})$ is like $\\mathbb{R} \\times \\mathbb{R}$ with pointwise addition and multiplication. It is easy to see that the only maximal ideals are $ I = 0 \\times \\mathbb{R}$ and $J = \\mathbb{R} \\times 0$. Note that there are many nonzero continuous functions belong to $I$ but not to $J$ and vice versa. In general this is true for any finite discrete topological space. But even when $K$ is not discrete the similar is also true. When $K = [0, 1] \\cup [2, 3]$, the ring of continuous functions from $K$ to $\\mathbb{R}$ looks like $C([0, 1]) \\times C([2, 3])$. This time by continuity if a function $f$ is not in $M_{p_0}$ for some $p_0$, there must be a family of $p$ such that $f$ is also not in $M_p$. However, since there are 2 components in $K$, one can divide the set of maximal ideals into 2 disjoint subset: $I = \\{M_p$ where $p \\in [0,1] \\}$ and $J = \\{M_p$ where $p \\in [2, 3] \\}$. Now it is possible to have many nonzero $f$ belongs to ALL ideals in $I$ but not in ALL ideals in $J$, and vice versa. So\u0026hellip; it looks like if we can give a suitable topology to the set of maximal ideals, one can reflects the topology of $K$ \u0026#x1f615; for example for the case above when $K = [0, 1] \\cup [2, 3]$ we hope $I$ and $J$ are open and closed, in order to reflect the corr. components of $K$. Conversely, given a (commutative) ring $R$ with its set of maximal ideals $\\mathcal{M}$, it seems to be possible to study the algebraic structure of $R$ by studying how those maximal ideals interact. For examples: If $\\mathcal{M}$ \u0026ldquo;disconnected\u0026rdquo; (with a suitable topology) like we saw in (1), that suggests $R$ is \u0026ldquo;decomposable\u0026rdquo; to a product. If $\\mathcal{M}$ is a singleton then $R$ is a field If the intersection of any two maximal ideals eqauls $(0)$ Aluffi, P. (2009). Algebra: Chapter 0. American Mathematical Society. As a side note, Aluffi\u0026rsquo;s book is really awesome as a book for brushing up algebra. Assuming you had \u0026ldquo;installed\u0026rdquo; some basic algebraic objects in your mind (i.e. know some basic group, ring, field theory), reviewing many concepts/ results through the categorical lens makes a lot of things clearer and much unified.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFrom time to time when thinking about a problem it is useful to think about some related problems at once, cuz the best way to solve a problem is to understand its components well enough so that the way to the core of the problem reveals itself to you. This way you also learn more from it and are able to appreciate it more since you will get more \u0026ldquo;context\u0026rdquo; of it by thinking about its \u0026ldquo;periphery\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/mathematics/linking-up-ring-and-topology/","summary":"I am reviewing some fundamental algebra, and I just learnt something beautifully suggesting the connection of ring theory and geometry, which made me eager to learn some algebraic geometry (which I was not very interested when I was in undergrad). It is an exercise in Aluffi\u0026rsquo;s book1.\nLets\u0026rsquo; jump into it.\nThe problem Let $K$ be a compact topological space, and let $R$ be the ring of continuous real-valued functions on $K$, with addition and multiplication defined pointwise.","title":"Amazed by how ring theory can be linked up to topology"},{"content":"Long time ago I read a quote from Habu1(as shown in the screenshot):\nThe true talent is the ability to carry on with passion, even when it probably won\u0026rsquo;t pay off.\n(not translated word by word)\nI was a teenager and it did not make any sense to me. How could passion be the true talent? Look at those geniuses like Gauss, Mozart, John von Neumann! What they have done is not something that could have been done by normal people. True talent is what they have that made them do the amazing things, I thought.\nI was too young and too simple, sometimes naive,2 to understand it.\nTwo types of talent Of cause, I am not suggesting that talent in the traditionally sense (i.e. the ability to effortlessly achieve something average people have difficulty in) does not exist. In fact, it might play a big role in areas especially in arts, mathematics and other fields that requires great originality - since there are countless people who are incredibly interested in these fields and work incredibly hard, yet only a small portion of them can become \u0026ldquo;great\u0026rdquo; in their fields, this must be the effect of something other than interest and industriousness - which is argueably the effect of \u0026ldquo;talent\u0026rdquo;.\nOf cause having passion and working hard matters a lot too, but I could not see their values when there are so many extreme cases of when \u0026ldquo;talent\u0026rdquo; outplays everything.\nBut now as a grown man, I look at the people I know, I see some of them are much gifted than I am, and yet they chose to do something that completely wasted their talents, completely irrelevant with what they are really good at. Some of them tried to strech themselves to the limit and do something great, but gave up the midway.\nAnd that\u0026rsquo;s how I had the epiphany.\nLet\u0026rsquo;s introduce some terminology: Type 1 talent is the ability to do something without effort; it is the skills and knowledge you assimilate unconsciously. Type 2 talent is the ability to work consciously, to reach beyond your limit, to go through pain but still enjoy it.\nthe talent described by Habu is the second type, without which you would not reach self-actualization, would not have the chance to fully use your type 1 talent, would not be able to do amazing things that you could have done!\nType 1 won\u0026rsquo;t let you go beyond your innate limit (by definition!). You might be very gifted and are able to do many things effortlessly, but it can only take you so far. Of cause, if you are gifted enough, you can still have a great job or your own business, have a comfortable life and have a lot of fun. Without type 2 talent, you will never reach your true limit and will waste what God gifted you 3.\nA very very edge special case as an example to see how they work in action:\nYou must have once met some people who are really good at, say, a sport in your life. They know and use their bodies smoothly with their animal-like instincts. But most likely they did not become top athletes as they grow up. Why? because for most sports, for most people, at some point deliberate practice is needed. You might be gifted enough to do a 98% accurate move effortlessly, but a person who keeps trying to really understand how all the things work together and tries all means just to reach from 97% to 99%, will beat the person who does 98% but is unwilling to put in the conscious work to go beyond 98%. Going to 99%, no matter it is from 97% or from 98%, requires shifting away from your habbits and try new things, lots of trials and errors, paying attention to insanely-hard-to-notice details, lots of reflections and self-monitoring, lots of times beating your own ego\u0026hellip;\nWhich is tremendously painful. If you don\u0026rsquo;t love what you do.\nIt is love Although it might sounds like something a toxic lover would say, but love is usually exemplified by getting through pain voluntarily: if you are just doing something without effort and serious considerations, that is just your instinct, not your choice. Love is irrational, without conditions. So the ultimate exemplification of love is to choose a path that rational people would not choose, but you still want to choose that path.\nAnd to actualize your true potential is \u0026ldquo;irrational\u0026rdquo; - since, if you know you effort is likely going to be fruitful and worth to be put it, then you are still quite close to your comfort zone. So, to have the type 2 talent implies that you love what you do. You don\u0026rsquo;t know whether your effort will pay off, you don\u0026rsquo;t where it will lead you. Yet you still want to do it.\nThat is exactly the even when it probably won\u0026rsquo;t pay off part in Habu\u0026rsquo;s quote.\nSomeone told me (as a joke, but I think there was a big portion of truth in it) that \u0026ldquo;rational people would not do a pure maths PhD and try to pursue an academic career. If you are intelligent enough to do well in this career path you could become a much more \u0026ldquo;successful\u0026rdquo; person in many other careers\u0026rdquo;. I laughed real hard but could not deny it.\nSo I think the type 2 talent is the talent to love.\nEven Gauss, Mozart, John von Neumann and many other great people whose names are carved in history, they surely have the type 1 talent, but they all must have insane amount of love in what they did.\nNow when I see someone do great things, I do not see the \u0026ldquo;talent\u0026rdquo;, I see love.\nhttps://x.com/yoshiharuhabu/status/1547832322564968448 . The twitter post is from a few years ago, but the original comes from his book \u0026ldquo;決断力\u0026rdquo; in 2005.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKinda insider joke: https://media1.tenor.com/m/EotZY3wxyOUAAAAd/jiang-zemin-too-simple.gif\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI am not religious, but I couldn\u0026rsquo;t find a better way to articulate it. Plain \u0026ldquo;not fully extending your talent\u0026rdquo; would be dull here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/thoughts/true-talent-is-carry-on-with-passion/","summary":"Long time ago I read a quote from Habu1(as shown in the screenshot):\nThe true talent is the ability to carry on with passion, even when it probably won\u0026rsquo;t pay off.\n(not translated word by word)\nI was a teenager and it did not make any sense to me. How could passion be the true talent? Look at those geniuses like Gauss, Mozart, John von Neumann! What they have done is not something that could have been done by normal people.","title":"The true talent is the ability to carry on"},{"content":" Warning\nYou need to know some basic group theory terminology to appreciate(I hope you do) the following content.\nBurnside\u0026rsquo;s lemma Here is the statement of the Burnside\u0026rsquo;s lemma:\nLet $G$ be a group that actions on a set $X$. Denote $X^{g}$ the set of fixed points of $g$ i.e. $\\{x \\in X | g \\cdot x = x\\}$, then the number of orbits of the action is equal to $\\dfrac{1}{|G|}\\sum\\limits_{g \\in G} |X^g|$.\nLet\u0026rsquo;s persuade ourselves that this is true first Instead of rushing to an attempt to prove, let\u0026rsquo;s try to persuade ourselves that this is true first.\nIf we think of the action of $G$ on $X$ as like stirring the soup and try to mix the ingredient evenly, then the more orbits you have, the less evenly you mixed it (of cause inside the orbit it could be well mixed, but imagine the extreme case when there are too many orbits so all orbits are small).\nHow about the formula $\\dfrac{1}{|G|}\\sum\\limits_{g \\in G} |X^g|$ ? Since it is a sum divided by $|G|$, we can think of it as the average of something. What is that \u0026ldquo;something\u0026rdquo;? Fix a $g \\in G$, imagine an $x \\in X$ is a particle in the soup, then $x \\in X^g$ means $g$ did not move $x$, so your stirring did not really stir $x$ at all! The larger $X^g$ you have, the more particles you did not move, hence the less evenly you mix the soup. So what the lemma say is, roughly:\nHow unevenly you stir your soup is the average number of particles you did not move for one stroke of stirring.\nSounds pretty legit!\nHigh level plan of attack How can we prove this lemma? Let\u0026rsquo;s contemplate the concepts involved in the statement, see what the gap is, explore what related concepts are in the gap.\nWhat we have:\nthe size of the set of what a given $g$ does not move the size of $G$ i.e. the number of $g$ the number of orbits What we want: a good relation between them(more precisely, an equality as the lemma stated).\nWell, since the size of each orbits can be all different, and they can be quite arbitrary, we may eventually need a way relates the sizes of orbits with some of the quantities above, otherwise the number of orbits can\u0026rsquo;t seem to bridge to the sizes of things.\nNow, since counting orbits is counting things in $X$, you might want to think about some \u0026ldquo;good\u0026rdquo; set of $g$ for a fixed $x$. Since we are already considering a set of $x$ that a given $g$ does not move, maybe it is good to consider a set of $g$ that a given $x$ does not move - That\u0026rsquo;s how we can come up with the concept of stabilizer.\nIf you already know Orbit-Stabilizer theorem you would have very likely thought of it since the concepts look so closely related. But even if you don\u0026rsquo;t, it is still likely that you can figure out that you need Orbit-Stabilizer theorem (and figure out its proof by yourself as well) with the above motivation. I am also going to give a intuitive explanation of Orbit-Stabilizer theorem below (I keep it short since it is mostly the same as how everyone think of it when they learnt it the first time).\nSo in summary here is 3 thing we need:\nConnect the number of orbits with some sizes Orbit-Stabilizer theorem Counting $\\sum\\limits_{g \\in G} |X^g|$ in a different way Connect the number of orbits with some sizes How can we deal with the annoyingly arbitrary sizes of orbits? When we have trouble in unifying things with different sizes, the first thing to try is to normalize things by dividing by their sizes (think about normal vectors and how we construct orthonormal basis in linear algebra).\nWith this idea, it is not hard to see that $\\sum\\limits_{x \\in X} \\dfrac{1}{|\\mathcal{O}_x|} =$ number of orbits, where $\\mathcal{O}_x$ is the orbit of $x$. This is well defined since orbits are disjoint.\nOne \u0026ldquo;free\u0026rdquo; observation: Since we already considering the sizes of orbits, it is natural to see that the sum of sizes of all the orbits is equal to the size of $G$.\nOrbit-Stabilizer theorem Fix a $x \\in X$, then the size of the orbit of $x$ multiplied by the size of the stabilizer is equal to $|G|$.\nIntuitive explanation: think about $G$ \u0026ldquo;modding out\u0026rdquo; what does not move $x$.\nDifferent ways to count edges Here come the next part: how to count $\\sum\\limits_{g \\in G} |X^g|$ ? Let\u0026rsquo;s draw a bipartite graph such that $x$ connects with $g$ when $g \\cdot x = x$. For instance:\nNow, what is $\\sum\\limits_{g \\in G} |X^g|$? - we pick $g$ one at a time, look at how many $x$ is connected with it, finally add up those numbers.\nThis is the same as counting the number of edges in the graph.\nSo what is another way to count the number of edges in the graph? - we can also pick $x$ one at a time, look at how many $g$ is connected with it, finally add up those numbers. And notice that for a given $x$, the number of $g$ connected with it is just the size of the stabilizer: $|G_x|$.\nHence, we can see that $\\sum\\limits_{g \\in G} |X^g| = \\sum\\limits_{x \\in X} |G_x|$.\nCombine what we have, now it is obvious why Burnside\u0026rsquo;s lemma is true: the number of orbits is just the sum of $\\dfrac{1}{|\\mathcal{O}_x|}$ over all $x$, which is $\\dfrac{1}{|G|}\\sum\\limits_{x \\in X} |G^x|$ (by Orbit-Stabilizer theorem), which is $\\dfrac{1}{|G|}\\sum\\limits_{g \\in G} |X^g|$.\nEpilogue The first remark is that the above is not a rigorous proof, since we assumed $G$ and $X$ to be finite for the sake of intuitive explanation.\nThe second remark: After I figured out the proof by myself, I went to google if there are any intuitive explanation online. Seems that none of the articles/discussions I found (the first-few-ish search results) provide a explanation with intuitive AND ELEMENTARY flavour. Although some people did give explanation with \u0026ldquo;higer level\u0026rdquo; math, such as Tao1.\nhttps://mathoverflow.net/questions/50033/intuitive-explanation-of-burnsides-lemma\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/mathematics/a-more-intuitive-explanation-of-burnside-lemma/","summary":"Warning\nYou need to know some basic group theory terminology to appreciate(I hope you do) the following content.\nBurnside\u0026rsquo;s lemma Here is the statement of the Burnside\u0026rsquo;s lemma:\nLet $G$ be a group that actions on a set $X$. Denote $X^{g}$ the set of fixed points of $g$ i.e. $\\{x \\in X | g \\cdot x = x\\}$, then the number of orbits of the action is equal to $\\dfrac{1}{|G|}\\sum\\limits_{g \\in G} |X^g|$.","title":"A more intuitive explanation of Burnside's lemma"},{"content":"Problem Let $I_{m} = \\int^{2\\pi}_{0} \\cos(x)\\cos(2x)\\dots\\cos(mx) dx$. For $m$ in $1, 2, \\dots, 10$, for which $m$ is $I_m \\neq 0$ ?\nSolution $I_m$ is non-zero if and only if $m \\equiv 0, 3 \\pmod{4}$.\nMain idea The official(seemingly? It is in the putnam problem book and a few solutions I found online do the same.) solution is to substitute $\\cos x = \\frac{e^{ix} + e^{-ix}}{2}$ followed by grouping the terms into $\\cos x \\cos (2x) \\dots \\cos (mx) = e^{\\text{something}}$, and analyze the something. This approach of cause is of cause good in its own right: it is exactly how you split up the $e^{in\\theta}$ terms into triginometric polynomials (or ) in Fourier analysis 101. However it is a little bit annoying to manipulate those powers of $e$. And for this problem it could be done much in a much simpler and elementary way: exploiting the point of symmetry.\nLet $f_m(x) = \\cos(x)\\cos(2x)\\dots\\cos(mx)$ and $g_m(x) = \\cos(mx)$. Notice that $g_m(x-\\pi) = -g_m(x)$ if $m$ is odd and $ = g_m(x)$ if $m$ is even. Hence $f_1(x-\\pi) = -f_2(x)$ , $f_2(x-\\pi) = -f_2(x)$ , $f_3(x-\\pi) = f_3(x), \\dots$\nSo $\\int^{2\\pi}_{0} f_m = \\int^{2\\pi}_{\\pi} f_m + \\int^{\\pi}_{0} f_m = \\int^{\\pi}_{0} (af_m + f_m)$ by change of variable, where $a$ is $-1$ when $m$ has a value such that $f_m(x-\\pi) = -f_m(x)$ and is $1$ otherwise.\nIf we note $0$ when the integral gives $0$ and $1$ if not, then it looks like:\n$$ m = 1, 2, 3, 4, 5, 6, 7, 8, \\dots $$\n$$ \\int f_m = 0, 0, 1, 1, 0, 0, 1, 1, \\dots $$\nIn general, the integral of $f$ is non-zero if and only if $m \\equiv 0, 3 \\pmod{4}$.\nThe $x-\\pi$ thing is obvious from staring at the following graphs:\nThought process Here I try to recall my thought process when I solve this problem, which is in general a good thing to do after solving a problem (reflect on the process instead of moving right on to the next thing). It will be less organized and more in a dialectical way:\nWhat is this problem about? Some weird integrals and some of them are supposed to be zero. I need to find out which are zero and which are not. The integrals are too awkward, don\u0026rsquo;t try to use trigo multiplying formulas. What is important? The integrals are awkward, but I don\u0026rsquo;t need to evaluate their values, only need to know whether they can be zero. How? Okay, I know integrating cosine from 0 to 2pi gives you zero. But that does not scale to the products of cos(mx). Let\u0026rsquo;s see why exactly integrating cosine from 0 to 2pi gives you zero. Do not use the antiderivative argument since that does not scale too. Okay cos is an even function, it is symmetric along 0. And it is periodic, so you can shfit the domain to be like integrating from -pi to pi, and boom, easy to see that it is zero. But what is the role of symmetry here? Merely symmetry does not anything about the value of the integral, especially doesn\u0026rsquo;t suggest that it should be zero. Okay, it is also symmetric along pi. But the good thing about it is that the \u0026ldquo;areas\u0026rdquo; cancel each other and that\u0026rsquo;s why the integral of cos x is zero. Accurately cos(x-pi) = -cos(x). So what? that means $\\int^{2\\pi}_0 \\cos(x) = \\int^{2\\pi}_{\\pi} \\cos(x) + \\int^{\\pi}_0 \\cos(x) = \\int^{\\pi}_0 -\\cos(x) + \\int^{\\pi}_0 \\cos(x) = 0$ Now this can scale to cos(mx): for a given m, all we need to know is its behavior under shifting the graph by pi (and see whether it gives you a minus sign or not). Done! I only need to write down the pattern of the sign flipping now. ","permalink":"http://localhost:1313/blogs/mathematics/elementary-solution-of-a-weird-integral-problem/","summary":"Problem Let $I_{m} = \\int^{2\\pi}_{0} \\cos(x)\\cos(2x)\\dots\\cos(mx) dx$. For $m$ in $1, 2, \\dots, 10$, for which $m$ is $I_m \\neq 0$ ?\nSolution $I_m$ is non-zero if and only if $m \\equiv 0, 3 \\pmod{4}$.\nMain idea The official(seemingly? It is in the putnam problem book and a few solutions I found online do the same.) solution is to substitute $\\cos x = \\frac{e^{ix} + e^{-ix}}{2}$ followed by grouping the terms into $\\cos x \\cos (2x) \\dots \\cos (mx) = e^{\\text{something}}$, and analyze the something.","title":"An elementary solution of a weird intergal problem (Putnam 1985 A5)"},{"content":" Info\nUpdates on 20250408:\nFixed a bug in parsing memory operand. Improved error message displaying.\nI happen to have to learn (just a bit of) RISC-V assembly, so I made a simple risc-v instructions explainer so that I don\u0026rsquo;t have to do multiple google searches whenever I have troubles in reading some risc-v instructions.\nIt can fully run in the browser, feel free to try it out at https://katsuragicsl.github.io/riscv-explainer/.\n","permalink":"http://localhost:1313/blogs/misc/riscv-explainer/","summary":"Info\nUpdates on 20250408:\nFixed a bug in parsing memory operand. Improved error message displaying.\nI happen to have to learn (just a bit of) RISC-V assembly, so I made a simple risc-v instructions explainer so that I don\u0026rsquo;t have to do multiple google searches whenever I have troubles in reading some risc-v instructions.\nIt can fully run in the browser, feel free to try it out at https://katsuragicsl.github.io/riscv-explainer/.","title":"[Updated 20250408]RISC-V explainer"},{"content":" Previous Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. ","permalink":"http://localhost:1313/papers/connectedness/","summary":" Previous Next \u0026nbsp; \u0026nbsp; / [pdf] View the PDF file here. ","title":"CONNECTEDNESS: A DIMENSION OF SECURITY BUG SEVERITY ASSESSMENT FOR MEASURING UNCERTAINTY"},{"content":" Info\nUpdates:\nAfter giving a second thought on the topic and reorganizing the materials, I had a sharing session with my teammates and decided to update this article accordingly. Updates include more suitable examples and graphics.\nInfo\nUpdates 2:\nIt is revised again and published as a preprint. Now you can see it at https://arxiv.org/abs/2503.17813 or https://katsuragicsl.github.io/papers/connectedness/\nAn empty business lingo or a good quantification? We hope to, and probably need to, quantify the severity of security bugs. The “need” mainly comes from the fact that we have to provide an interface to management: We need to have a reasonably objective quantification to tell how serious an issue is (and show that we would not overplay/downplay something for our own self-interest), and that would lay the groundwork for the establishment of a bug handling standard that the engineering team would follow.\nBut do our quantifications accurately reflect the severity of the bugs? Or do they reduce themselves to merely buzzwords and props for business talks?\nBusiness lingo usually does not match the aesthetics of technical people: sometimes they are even allergic to the vocabulary of business talk so much that they want to escape from it, like Nero in the traders\u0026rsquo; story in Fooled by randomness1 (except when they have to boast or to look for jobs on LinkedIn, of course, since after all we have to feed ourselves right? \u0026#x1f60f;). So when something annoys technical people and they complain about the technical inaccuracy, most of the time it is because the thing contains a bit too much business lingo.\nAs the CVSS scoring system2 evolves over these years, it becomes more and more complicated, and yet still addressing the severity of bugs in a not very ideal way. I and my colleagues have ranted about it, curl’s author also complained about it - I believe there are more people out there who are not happy with it.\nWe also have the P-level system: like the one of Bugcrowd which divides the severity of bugs into 5 levels, according to the bug types. This system gives us more flexibility on some occasions, especially when the severity is “varies” in the taxonomy. But that does not make things clearer, it just sweeps the messy part of the problem under the carpet: instead of stating what is it that determines the actual severity in such cases, it leaves the judgment to the free evaluation of a blackbox (the one who makes the final decision) and trusts the outcome. To be fair, usually it is not a serious problem to those who have solid experience in dealing with vast varieties of security bugs, but in this case, instead of accusing it of being an inaccurate/ambiguous scale, I will try to expose the hidden concept we used when we are making such decisions, which is the main goal of this article.\nNot all P5 are equal Let’s start with an imaginary yet realistic example:\nConsider two bugs. Bug 1 is a “Missing Secure Cookie Attribute”. Bug 2 is a Weird input reflection: the input in a GET parameter is splitted into 2 parts and reflected into 2 different places in the same page, but no one could find a valid XSS, and sanitization is in place.\nNow bug 1 is a standard low severity issue, in fact, in practice it is quite often to be treated as informational. For bug 2, it falls under the category of a standard informational issue (user input reflection) although its behavior is not quite standard among the issues in that category.\nSo in reality they will both be treated as with roughly the same severity, the input reflection one will probably be lower.\nHowever, if you could only fix one of them, which one would you choose? (In reality you could choose \u0026ldquo;both\u0026rdquo;, but let\u0026rsquo;s pretend, for the sake of thought experiments).\nI would choose the input reflection one. Let\u0026rsquo;s do some analysis. The core idea of CVSS and the usual practice of how people evalute severity today can be summarized as calculating the ratio of \u0026ldquo;how hard/likely to exploit\u0026rdquo; to \u0026ldquo;how bad is the impact\u0026rdquo;. Let\u0026rsquo;s see the comparison on \u0026ldquo;how hard/likely to exploit\u0026rdquo;:\nCookie without secure flag Input reflection Need to have the target website allow HTTP Need a way to bypass sanitization Need to trick the user to click on HTTP link Need a way to trigger the payload correctly after the input split Need MITM to capture the cookie Need user to click the link for the reflected XSS For the impact part:\nCookie without secure flag Input reflection Stealing the cookie (you get the MITM) Stealing the cookie Leaking the cookie to whoever has MITM Further phishing You would probably agree that their \u0026ldquo;how hard/likely to exploit\u0026rdquo; to \u0026ldquo;how bad is the impact\u0026rdquo; ratio look quite close.\nHowever, if you break down how exactly are they being \u0026ldquo;unlikely to happen/hard to exploit\u0026rdquo;, you will have a different point of view. We can do so by asking questions like \u0026ldquo;how do they fit in to a bigger picture?\u0026rdquo;, \u0026ldquo;how can I manipulate it in a different way?\u0026rdquo;. So here is another table:\nCookie without secure flag Input reflection Only relevant to cleartext transmission issue between the user and the web server Not linked to many different things There might be a way to trigger the input split in an useful way The sanitization library might have bugs If there are subdomains that gets data from the site which has the reflection issue, and they do some \u0026ldquo;weird\u0026rdquo; manipulation to the data\u0026hellip; There are way more ways to play with the input reflection issue, it is connected with more things.\nThe “use cases” of the reflection issue is all extremely hypothetical. But they cast more shades over our analysis: it feels like there is a black swan in one of the corners in the world, you just know it is unlikely to be the next corner you are going to turn.\nMeanwhile, dealing with the Cookie Secure flag issue is (almost) like walking on a straight road. You see a fierce dog in front of you which look like going to bite you if you come close. Well Ok, I can just stop walking. Nothing scaring hiding in shadow.\nBut how do we articulate the analysis above? What is the concept behind it?\nCentrality I first learned the concept of centrality from Timothy Gowers\u0026rsquo;s talk on 2022 Fields Medal Symposium3. He used centrality (as one of the factors) to explain why we consider some problems to be more “interesting” than others. I found that this concept can be applied in similar ways to explain several things out of the mathematical realm, including how we perceive the severity of a bug.\nIf you plot out all the \u0026ldquo;connections\u0026rdquo;, the more connections a thing have, the more they will look more “central” in the graph:\nA more abstract example:\nImagine you need to flow some water from the sources to the end like in the graph below:\nWhich situation triggers you more? Is it when the “center” node has a minor dysfunctioning:\n\u0026hellip;or when the end node has a minor dysfunction?\nIn the case of the dysfunctioning worsening/getting “exploited”, both case stops water flow from the source to the ends, but the center node case just looks more critical. Even if we assume the cost of fixing the issue is the same in both cases (say we can just click a button and a new node for replacing will be spawned like magic). But you probably feel more urge to look at the center node as it looks like there are more things you could dig into.\nCentrality causes a few things:\nBring uncertainty into play Hidden implications More flexibility (hence more likely to have some new ways to break known defenses) Also\u0026hellip;although it does not affect the severity, but it does courage us to pay more attention on things that have more centrality: more transferable lessons you will learn from dealing with it.\nHence, centrality is definitely one of the factors for evaluating the severity of a bug, the more centrality the more severe. However it is missed in the current quantification systems and not even mentioned by people although they might be using it in a daily basis.\nTaleb, Nassim Nicholas, 1960-. (2001). Fooled by randomness : the hidden role of chance in the markets and in life. Penguin Books.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAt the time of writing, the latest version is CVSS 4.0: https://www.first.org/cvss/v4-0/. I, as a security worker, find it a bit painful to use as well.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.youtube.com/watch?v=EcdW3i6psmI\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/security/centrality-what-cvss-missed/","summary":"Info\nUpdates:\nAfter giving a second thought on the topic and reorganizing the materials, I had a sharing session with my teammates and decided to update this article accordingly. Updates include more suitable examples and graphics.\nInfo\nUpdates 2:\nIt is revised again and published as a preprint. Now you can see it at https://arxiv.org/abs/2503.17813 or https://katsuragicsl.github.io/papers/connectedness/\nAn empty business lingo or a good quantification? We hope to, and probably need to, quantify the severity of security bugs.","title":"[Updated 20250331]Centrality: how we actually perceive the severity of a bug"},{"content":"After the huge attention gained by deepseek R1, I have seen a good amount of people immediately went to looking for censorships (which are expected to exist), laughed at it, and looked down on it. They totally missed the points and these behaviors made them look like slaves of their very own defense mechanism1.\nDeepseek did make some techonological breakthroughs. Indeed the censorship is a defect of such a great product. However making jokes on it with its censorships is just like catching grammatical errors in an insightful article and disdain the article because of that. Yes, grammatical errors are errors, catching itself could be useful, but doing that for attacking an insightsul article is another thing. It just misses the point.\nIf you see China as a rival, it is also like laughing at your enemy\u0026rsquo;s weapons just because they \u0026ldquo;look\u0026rdquo; funny.\nIf you want to disdain an insightful article, disdain by its ideas.\nThe accuses about cheating with OpenAI data are even more related and look less like jokes. (if ignore the validity of those accuses)\nAn even more related critiques can be pointing out the fact that it did not push the limit of AI reasoning ability (since it is roughly as good as o1), although it optimized the whole process. Of cause making training cheaper is important and it helps researchers to push the limits of AI as well (since the testing cost became lower as well). Depends on your interested areas, you could value Deepseek differently. \u0026lt;- This, is still way better than taunting it because of the censorships.\nSince they could not bare the fact that Chinese make breakthroughs that others did not make. This is called \u0026ldquo;glass-made heart\u0026rdquo; in Chinese, ironically mostly used in describing the hysterical behaviors of pro-CCP people when they heard something that is unfavorable to CCP.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/thoughts/picking-grammatical-mistakes-in-an-insightful-article/","summary":"After the huge attention gained by deepseek R1, I have seen a good amount of people immediately went to looking for censorships (which are expected to exist), laughed at it, and looked down on it. They totally missed the points and these behaviors made them look like slaves of their very own defense mechanism1.\nDeepseek did make some techonological breakthroughs. Indeed the censorship is a defect of such a great product.","title":"Picking grammatical mistakes in an insightful article"},{"content":"Just added support for tikzjax since I received some feedback about my hand-drawn diagrams are hard to read\u0026hellip;now I can use nice graphs like these:\nsource: https://tikzjax.com/ source: https://tikzjax.com/ source: https://tikz.net/ source: https://tikz.net/ ","permalink":"http://localhost:1313/blogs/meta/added-tikzjax-support/","summary":"Just added support for tikzjax since I received some feedback about my hand-drawn diagrams are hard to read\u0026hellip;now I can use nice graphs like these:\nsource: https://tikzjax.com/ source: https://tikzjax.com/ source: https://tikz.net/ source: https://tikz.net/ ","title":"Added tikzjax support"},{"content":" Disclaimer: The author\u0026rsquo;s knowledge about the legal system by no means applies to anywhere outside of his homeland. But what is legal or illegal should be similar enough for the discussion below - at least for first world countries, I guess.\nThat said, the author is not a lawyer and does not have accurate knowledge of any concrete laws. But one does not need to know (too many of) them for this article.\nThose jerks we see everyday Jerks are not criminals. Maybe they do break some laws in some occasions, but the \u0026ldquo;crimes\u0026rdquo; are not serious.\nJerks are those who commit offences to people near them. They do things that are barely legal but extremely annoying and even morally wrong. They know what they are doing, and they refuse to change, they got the brass neck to do awful things.\nThey are those who talk or play music really loud on a busy train, or those who occupy unnecessary space or standing unnecssarily close to annoy people, those who say roundabout and mean things (but right in your face), or those take a little advantages of you and act like he is right.\nThey are annoying not because they give you offences, they do give you offences, but the annoying part is that you don\u0026rsquo;t have a good option to make them pay without hurting yourself.\nYou options look like this:\nThe red (same as the color of the word \u0026ldquo;the jerk\u0026rdquo;) numbers in the boxes represent the gain/ loss, or how happy will the jerk be in different cases. The blue numbers are the counterparts for you. Since what the jerks do are not something seriously hurting your benifit, if you suck it, you will get let\u0026rsquo;s say $-5$. If you fight back, for examples by condemning them verbally or by any form or force (moving them out of what they should not be at; stopping whatever physical actions they are doing; or even giving them a big slap), it does offend them back, but it is also highly likely going to cost you more. Let\u0026rsquo;s say it costs you $-10$. (Note that this analysis works only for good citizens, normal people who have to go to work tomorrow so that they can feed themselves and their family. Otherwise you might be more willing to take the consequences of escalating the scene and hence more willing to fight back. We will discuss only in the case of that we are normal people.)\nSome people might think there should be some ways to fight back without too much cost. I will list a few of them that I could imagine, and explain why they are bad ideas. Honestly, if you are neither a bad person nor a saint nor are pampered, you will agree with me.\nCondemn them. Most of the jerks are thick-skinned. You only irritate yourself. Film a video of what they did and put it on Youtube or some Facebook groups They do get some attention and shitstorms among a small group of people, but soon they will be forgotten. Do you remember the face of the person you see the last such video? I don\u0026rsquo;t. I bet you neither. Maybe those who know them will recognize them and remember their behaviors much longer. But if they are kind and loyal enough to those who are close to them, or more accurately, those who really matter to them, they will likely be forgiven by those people. And they will still have a good life.\nSome people believe that those who do jerky behaviors to them must be jerky to everyone, and hence they will get freezed out. But that is not true. They can be a great spouse, a great parent, a great son/ daughter. \u0026ldquo;Clever\u0026rdquo; jerks know who to bully and who not to. See the next section for more. Another problem is that there are some jerky behaviors that you wouldn\u0026rsquo;t have the time to film a video or take a picture. Such as bumping people on the street for no reasons and walking away extremely fast. (Sidenote: if you are a big guy you can do this for \u0026ldquo;good purposes\u0026rdquo; sometimes. But it only works in very few occasions and does not work at all for someone not physically strong.) Call the police. Meh. Most jerky behaviors are barely legal as we said. Cops are either unable to or not willing to do much. So, the \u0026ldquo;best\u0026rdquo; option here is to suck it, just like what most people do. What is so bad is that, the \u0026ldquo;best option\u0026rdquo; is always letting the good people \u0026ldquo;lose\u0026rdquo; and the jerks \u0026ldquo;win\u0026rdquo;. The rationally best choice gives the shittiest outcomes.\nUnderstand that facing criminals is a differnt story. If someone is commiting crimes against you, you won\u0026rsquo;t be bothered by the same problem. You can legally fight back. And if the crime is serious enough, you won\u0026rsquo;t hesitate to pay the cost of time, effort and money to fight back by any means, if you are able to do so. Of cause most of the time you don\u0026rsquo;t even have the ability to fight back immediately in the case of serious crimes, but that is another issue. For example, when someone is commiting physical assault on you, assuming you are 50/ 50 with him in a fight, then you options will look like this:\nA totally different situation.\nMorals does not work for strangers Certainly the legal system is not the only reward and punishment system in our society. Morals is a strong force to restrict people\u0026rsquo;s behavior\nMoral system works well for a group of people who know each other and will live with, or work with, or rely on, for a significant period of time, like colleagues, families, close neighbourhood especially in rural areas. If you act unethically and get freezed out by those people, you will have a really hard time. Especially in rural areas, that would makes the whole village unlivable for you.\nHowever, morals does not work for strangers. If someone is almost sure that he/ she will see you only once in a lifetime, he/ she can literally do anything to you without consequences (except the legal consequences). There is nothing we can do about it. You can condemn those who are not kind-hearted and curse them with the ugliest words you know, nothing would happen in reality, except that you are damaged and angry.\nThis is just like the difference between normal prisoners\u0026rsquo; dilemma and prisoners\u0026rsquo; dilemma with infinite or unknown number of rounds. If there is only one round (you only see each other once in a life time), the \u0026ldquo;best\u0026rdquo; strategy is to betray. Only when you don\u0026rsquo;t know when will the relationship with someone end, there is possibility to cooperate.\nIt sounds horrible to those innocent, but that is how it works.\nThe worse thing is that, morals of societies that are more\u0026hellip;uh, barbaric, actually flavors jerks less (Of cause barbaric societies have other problems). Jerks enjoy more advantages in civilization.\nMorals from so-called civilized world makes it worse I forgot where did I see a sarcastic line about respects, not the exact wordings but more or less the same meaning:\nCavemen know repects better than us. Cuz if you don\u0026rsquo;t respect someone you will be still be fine. But if a caveman doesn\u0026rsquo;t repect another caveman, his head will probably be cut in half.\nThis is a bit exaggerated, but it does not hurt the fact modern world overly vilifies and oppresses violence.\nWhat if minor violence against jerky behaviors is widely acceptable and is free from moral condemnation? Perhaps let\u0026rsquo;s get a bit extreme to show the effect: if such thing is even encouraged in our moral system, so most people are tend to fight back to make the jerks pay, now the jerks need to consider between acting like a jerk or not to do so.\nIn this case, not acting like a jerk will be more attractive.\nA sidenote is that, undoubtedly violence out of control creates other problems. The solution proposed by civilization is that we all should agree to \u0026ldquo;outsource\u0026rdquo; our rightful violence usages to the law enforcement, so that the violence is easier to manage in theory, and cough HOPEFULLY cough we can ensure that the law enforcement use force the right way.\nThe bottomline is, moral system can\u0026rsquo;t save you from jerks. It even flavors them.\nIt is an unfair game I hope I have demonstrated that how our laws and moral beliefs put them in such a situation that doing jerky things to strangers are flavored. It puts us into an unfair game - they have the option to give you (minor) offences with almost zero cost, but your other options always cost you more than that, and they are always winning.\nThe very reason we hate jerks is not how bad they are, but that they are just bad enough to irritate and causes minor damages, but \u0026ldquo;harmless\u0026rdquo; enough to get away from consequences so often. It is always the unfairness.\nRationality As a single person, there is not much you can do. If you are a \u0026ldquo;rational\u0026rdquo; (which means you always pick the choices that bring you the most benifits) person, your \u0026ldquo;best\u0026rdquo; choice is still to suck it. Things will only change in either case:\nThe legal and moral systems change so that they do not flavor jerks anymore. You give up to be a \u0026ldquo;rational\u0026rdquo; person. In fact if everyone is \u0026ldquo;rational\u0026rdquo; the legal and moral systems change will not change. It is always some \u0026ldquo;insane\u0026rdquo; people who act against their own interest and change the tide. Of cause it does not mean that you can go on the street to slap a jerk and the whole world will change overnight and you don\u0026rsquo;t have to be arrested. No one know when will a change comes or will it even come.\nAs a sidenote, studying how \u0026ldquo;irrational\u0026rdquo; choices can bring good result is also an interesting topic.\n","permalink":"http://localhost:1313/blogs/thoughts/our-legal-and-moral-system-flavors-jerks/","summary":"Disclaimer: The author\u0026rsquo;s knowledge about the legal system by no means applies to anywhere outside of his homeland. But what is legal or illegal should be similar enough for the discussion below - at least for first world countries, I guess.\nThat said, the author is not a lawyer and does not have accurate knowledge of any concrete laws. But one does not need to know (too many of) them for this article.","title":"Our legal (and moral) system flavors the jerks"},{"content":"Why ignore 50% of what we know about the data1 Everyone learnt binary search in Algo 101. It is the fastest way (among comparison based searches) to find an element in a sorted array. All you need to carry out the algorithms is the comparison between the target element and the element at the current position. It is widely applicable because it assumes so little from the data. But for many real life problems, we do often know something apart from merely the comparison between the two numbers. For examples:\nLooking for the phone number of a friend from the contacts Picking an e-book from a folder by its name Sure, nowadays no one uses paper phonebooks anymore and you can instantly get the number you want by the built-in search feature. But you get the idea, let\u0026rsquo;s pretend that there are no search features for them. And I bet you have been in some cases where you need to look things up in a big repository but cannot use search, like dumpster diving some old and awkward file sharing sites.\nIn the above cases, we do know something about the data: The name (in English, let\u0026rsquo;s say) of the target and that of the current object I am looking at. Assume I want to look up the phone number of a friend named \u0026ldquo;Stuart\u0026rdquo;, then, at the very first step I would not bother to look at the first few entries in my contacts, unless I have a lot of friends whose names start with \u0026ldquo;S\u0026rdquo; and none of them are like \u0026ldquo;Ben\u0026rdquo;, \u0026ldquo;Ken\u0026rdquo;, \u0026ldquo;Peter\u0026rdquo;, \u0026hellip;, right? I would likely just scroll down to the bottom of my contacts and start there. If I see names like \u0026ldquo;Tim\u0026rdquo;, \u0026ldquo;Teddy\u0026rdquo; there, I would scroll up JUST A BIT, but not too much, since \u0026ldquo;Stuart\u0026rdquo; are supposed to be quite close to these names.\nI guess the above process would be very natural to anyone. Its rationale lies on the information we have for the expected distances between elements: When we know the concrete value of the current element and the distribution of the elements, we can guess the location of our target.\nSo, if about the data we do know more than just their comparisons, why ignore what we know and blindly apply plain binary search? In binary search we don\u0026rsquo;t have such knowledge, so taking the middle is the \u0026ldquo;safest\u0026rdquo; thing to do; but when we have such knowledge, we can be more aggressive.\nOf cause, in real life we usually don\u0026rsquo;t know what the distribution really is, so we can only guess. Othewise we should be able to hit the target with 1 step, by calculating the exact position of the target. Let\u0026rsquo;s start with the simplest case: we assume the distribution to be uniform, i.e. for any given array $A$ with length $l$, $A[1] = x$, $A[l] = y$, we assume the values from $x$ to $y$ distribute uniformly within the $l$ elements. So if we want to find $k \\in [x, y]$, we should look at the element at the index $$1 + \\dfrac{(l-1)(k-x)}{y-x}$$ since the values between $x$ and $y$ should be distributed evenly in $A$.\nThe arithmetics $A[l] - A[1]$ above makes sense since we can just assign numbers to the elements once we assume there distribution. For instance assigning $0$ to $a$ and $25$ to $z$ in the case where the data are single lowercase alphabets should do the trick.\nExample: We have $10$ cards, each of them are numbered with an integer within the range from $1$ to $10$ (duplicates possible) on the back side. You can pick one card and turn it over each time. They are sorted. Find $4$.\nObviously we should try the 4th card first. The 4th card is numbered with $2$. Now we can do the same to the 4th to 10th cards. The possible range now is from $2$ to $10$. So we should check the 5th or 6th card. Let pick the 6th. The 6th card is still $2$. Now we can do the same to the 6th to 10th cards. The possible range now is from $2$ to $10$. So we should check the 7th card. The 7th card is $4$. Bingo! Time complexity analysis Warning: the analysis below is very hand-wavy.\nWorst case: Obviously, the worst case is that our guesses are wrong every time. It will takes $O(n)$. For example when we have the possible values from $1$ to $10$ but an array like this: $1, 1, 1, 1, 1, 1, 1, 1, 2, 10$, if we want to find a $2$ in this array, we will have to go through all elements.\nAverage case: (assuming the data is in uniform distribution) Our intuition tells us that it should be faster than binary search, afterall we made use of significant information which binary search doesn\u0026rsquo;t use to narrow our searches. Should be faster than $\\log n$.\nAnother useful intuition is that our guesses should be \u0026ldquo;quite close\u0026rdquo;. Otherwise the distribution would not be \u0026ldquo;uniform enough\u0026rdquo;. So how wrong could our guesses be? This question leads us to estimate the expected value of $|K - k_i|$ where $K$ is the index of the target and $k_i$ is our estimated index in the $i^{th}$ round.\nSo our intuition is saying that this expected value should be small, because $K$ \u0026ldquo;should\u0026rdquo; be at exactly $k_i$ instead of shifted to somewhere else - we can think of $K$ as a binomial random variable with expected value $k_i$, i.e. $B(k_i, p_i)$ with $p_i = \\dfrac{(l_i)(k_i-x_i)}{y_i-x_i}$. (here $l_i$ is the length the array in the $i^{th}$ round. $x_i$ and $y_i$ are the values in the boundaries.) Hence we have: $$E[(K - k_i)^2] = l_ip_i(1-p_i) \\leq \\dfrac{l_i}{4}$$\nSo the expected value of the square of $|K - k_i|$ is less than $\\dfrac{l_i}{4}$, which means we should expect $|K - k_i| \\leq \\dfrac{\\sqrt{l_i}}{2}$. So roughly speaking our guessed index should drop onto somewhere in $[K - \\dfrac{\\sqrt{l_i}}{2}, K + \\dfrac{\\sqrt{l_i}}{2}]$.\nSo each round we \u0026ldquo;chop\u0026rdquo; the array at one of the boundary of $[K - \\dfrac{\\sqrt{l_i}}{2}, K + \\dfrac{\\sqrt{l_i}}{2}]$. We could expect every 2 rounds the array shrinks from being with length $l_i$ to being with length $\\sqrt{l_i}$.\nThat means our problem is broken down to a simlilar problem with size $\\sqrt{l_i}$ and the process should ends when $\\sqrt{l_i} \\leq 2$ i.e. when you only got one choice of index to check in the next step. Some simple calculation tells us that when $i$ reaches $\\log{\\log{l}}$ the said condition will be true.\nSo the average case complexity should be $O(\\log{\\log{l}})$.\n(Note that the reasoning above is very far away from being rigorous, it just helps us to guess the answer. I did not manage to prove it.)\nThe wheel we have reinvented Well, turns out that the algorithm above is just interpolation search, nothing new.\nOf cause since I did not manage to prove the time complexity which is the hardest part, I cannot claim any credits (more accurately, I cannot do so even if I figured out the proof). But I did find a proof by Y Perl2. That proof was complex.\nI just decided that things like this (and the previous blog post about reinventing the Catalan numbers) which rediscover established results by rethinking \u0026ldquo;101\u0026rdquo; topics should go into a series called something like \u0026ldquo;Wheels reinvention\u0026rdquo;.\nAs a sidenote, if we go the other way around and try to actively reverse engineering the discovery process of something we already knew, that will also be super fun\u0026hellip; I should definitely try it.\nMore general cases? Of cause, in real life no one would commit into a not-so-accurate guess and carry through it without a single change: throughout the process of checking the elements in the inferred positions, we gradually learn the actual distribution of the data and hence can adjust our expectation. For example, if you keep getting friends with name start with \u0026ldquo;L\u0026rdquo; when you try to find \u0026ldquo;Mike\u0026rdquo;, then probably your friend list skewed towards the first half of the alphabets (i.e. you got a lot of friends with names like \u0026ldquo;Angela\u0026rdquo;, \u0026ldquo;Bruce\u0026rdquo;, but less like \u0026ldquo;Pam\u0026rdquo;, \u0026ldquo;Veronica\u0026rdquo; etc.), especially those start with \u0026ldquo;L\u0026rdquo;. In this case we would like to adjust our assumption so that we put more \u0026ldquo;weights\u0026rdquo; to input starts with \u0026ldquo;L\u0026rdquo; - so from the current element (let\u0026rsquo;s say \u0026ldquo;Luigi\u0026rdquo;) we will skip more elements to look for \u0026ldquo;Mike\u0026rdquo;.\nA typical flow of the process above would be:\nWe assume all names are equally possible to appear in my contacts. The friend I want to look up is \u0026ldquo;Mike\u0026rdquo;, it is expected to be in the middle. We check the middle, it is \u0026ldquo;Luigi\u0026rdquo;. So next we check the, let\u0026rsquo;s say $0.1^{th}$ position of the remaining array We got \u0026ldquo;Luisa\u0026rdquo;. Well I guess we have a lot of friends with names start with \u0026ldquo;L\u0026rdquo;. Maybe next we should check the $0.2^{th}$ position if the uniform distribution says it should be $0.1$ ? (I have to admit that I don\u0026rsquo;t know much people names start with \u0026ldquo;L\u0026rdquo; but after \u0026ldquo;Luisa\u0026rdquo;\u0026hellip; but whatever) We got \u0026ldquo;Mila\u0026rdquo;. We\u0026rsquo;ve gone to far. If we assume uniform distribution we should check $0.9$. But remember there are a lot of \u0026ldquo;L\u0026rdquo; friends, so it should be like $0.95$. and so on\u0026hellip; Things go a bit hand wavy at this point. Rigorously defining this generalized algorithm and analyzing it will likely take good effort.\nThis is teasing one of the most famous quotes in the Brazilian JiuJitsu world. https://www.youtube.com/watch?v=qrbnGmdnHOg\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYehoshua Perl, Alon Itai, and Haim Avni. 1978. Interpolation search—a log logN search. Commun. ACM 21, 7 (July 1978), 550–553. https://doi.org/10.1145/359545.359557\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/computer-science/why-use-binary-search-when-you-can-guess-where-it-should-be/","summary":"Why ignore 50% of what we know about the data1 Everyone learnt binary search in Algo 101. It is the fastest way (among comparison based searches) to find an element in a sorted array. All you need to carry out the algorithms is the comparison between the target element and the element at the current position. It is widely applicable because it assumes so little from the data. But for many real life problems, we do often know something apart from merely the comparison between the two numbers.","title":"Why use binary search when you can guess where it should be"},{"content":"Ladder of abstraction Well the title is obviously a tweak of the \u0026ldquo;ladder of abstraction\u0026rdquo;. For those who does not know, \u0026ldquo;ladder of abstraction\u0026rdquo; refers to the classic essay by Bret Victor. I am kind of stealing this terminilogy for the reader to visualize climbing up and down on the ladder of abstraction to make it easier to explain the nature of different jobs. The main question this blog post deals with is:\nWhy do some people love doing X but hate to do X as a job, complaining about having to catch up with trends/ news? And my answer is those people are working at the \u0026ldquo;wrong\u0026rdquo; level of abstraction of their fields.\n\u0026ldquo;I love tech\u0026hellip;but I hate my tech career\u0026rdquo; As a person working in security, I used to be confused when I hear people complaining how much they have to keep themselves updated and learn the latest tech stacks, the typical quotes of them are:\nI spent X years to learning programming language/ framework A, and now I have to learn a new language/ framework B or I will be replaced by those young devs. I hate this.\nIt is exhausting to work in security as you have to keep up with the latest news such as XYZ company is hacked\u0026hellip;\n(it might be unbelievable for you the reader, since if you are reading this obviously you are very different from the crowd, but trust me, there are a bunch of such people).\nAre they not curious people? Do they not enjoy the technical things they are working on? I used to think so, I thought it is just about personalities, but I was wrong. They ARE curious people and they do enjoy the things they work on, what they complain about is actually the level of details they have to handle.\nThe guy who complains about having to learn new programming language, he does enjoy programming (well, at least does not hate it), what he hates was the requirements of actually implementing a software while keeping up with the trends. I would boldly say that he should do competitive programming instead of developing CMS systems for his clients. To work as a developer and implement CMS systems is working at a very \u0026ldquo;low level\u0026rdquo; of the abstraction ladder - you have to take care of virtually every bits of the software you create, including which languages/ frameworks to use, due to technical or non-technical reasons (Maybe a new language/ framework does give better performance, maybe the market just want to see something that is \u0026ldquo;up to date\u0026rdquo; and flashy).\nThe lower you are working at the abstraction ladder, the faster the world around you changes, and you have to keep up to that. However to do competitive programming, no one cares which language do you use, all you do is almost just to think about how to manipulate the data, if there exists a machine that can run your pseudo codes, you can write pseudo code too - the language/ framework selection factor is abtracted from your task.\nFor those who complain about having to \u0026ldquo;keep up with the latest news\u0026rdquo; (probably working in SOC or similar), similar argument applies, perhaps they should be a researcher of malware techniques.\nOf cause, can doing competitive programming or researching malware techniques make a living for them is another story ツ, but the goal of this blog post is to reveal the true reasons of why are some people in kinda love/ hate relation with their professions and the \u0026ldquo;level of abstraction\u0026rdquo; embedded in the nature of many jobs.\nDon\u0026rsquo;t get me wrong - I am not suggesting that some work are superior than others, but different people do their best in different types of work, and if someone constantly complaint about his/ her job nature especially about the amount of details, perhaps they are working in the wrong level of abstraction, and they should be aware of their true feeling about the work. Perhaps they should look for a position that allows them to work at higher level of abstraction.\nAnd don\u0026rsquo;t forget that some people do really love to be drown in details by working at very low levels.\n","permalink":"http://localhost:1313/blogs/thoughts/level-of-abstraction/","summary":"Ladder of abstraction Well the title is obviously a tweak of the \u0026ldquo;ladder of abstraction\u0026rdquo;. For those who does not know, \u0026ldquo;ladder of abstraction\u0026rdquo; refers to the classic essay by Bret Victor. I am kind of stealing this terminilogy for the reader to visualize climbing up and down on the ladder of abstraction to make it easier to explain the nature of different jobs. The main question this blog post deals with is:","title":"Work at the level of abstraction that works for you"},{"content":"Relaxation as a result of mastery, instead of a prerequisites In many sports/ crafts that I have experienced, relaxation is considered by many the prerequisites of mastery:\nIn playing piano, your fingers, wrists, arms and shoulders have to be relax or you will find it extremely hard to press the right keys fluently; In online games that require fast reactions, rookies hands are stiff and they press buttons with unnecessary strength and range of motion, which stops them from pressing right buttons at the right time smoothly; In jiujitsu, being reasonably relax helps you to perform efficient moves, saving your energy and gives you more choices in different situations in which you have to adapt and react quickly. Mainstream arguments I have seen in the above fields are that relaxation is a prerequisite of getting good at these skills: \u0026ldquo;You have to relax or your chords sound horrible!\u0026rdquo;, \u0026ldquo;you have to relax to improve your techniques\u0026rdquo;. Yet most people have to go through a good amount of practice untill they become relaxed and start being good at those skills. Did they not understand how to relax their bodies when they were beginners? Everyone knows how to relax their bodies - just lie on a comfortable massage bed and you will be relax. The hard part is being relax while doing things that we are not good at.\nAnd I would like to argue that relax should come after mastery.\nWhen we first start to learning how to play piano, it feels like our fingers do not belong to us - it is almost an impossible task to control our fingers to work separately and yet cooperatively just to press the right keys without touching the wrong ones at the same time. Is it possibe to be relax at this stage? I am afraid not. It takes huge effort to tame your body to perform tasks that you are unfamiliar at. Being stiff is a sign of using unnecessary energy, which is totally normal when you don\u0026rsquo;t know the right muscles to use or unable to avoid using unnecessary muscles.\nBeing \u0026ldquo;relax\u0026rdquo; is the result of using \u0026ldquo;just right\u0026rdquo; amount of energy - using only the muscles that are needed at the moment. And this is impossible until you acquired the skills and can effortlessly control your body to perform the task - which means mastery. Here mastery doesn\u0026rsquo;t mean being a legendary pianist or a world champ in LOL or jiujitsu, it means you have mastered all the building blocks of your craft and you are ready for the journey of exploring the advanced usages of them, just like a baby finally learnt how to walk steadily - now he knows how to use his feet, so he can freely explore how to jump, how to run\u0026hellip;this is an important milestone of learning.\nThat said, one should notice that it is impossible to be relax before mastery - if you are a beginner in piano can you are completely relax, you are not trying to tame your fingers! You are not trying to make them to do the correct movement! You will inevitably be stiff if you try to do so. And in jiujitsu trying to be relax and look like a seasoned black belt toying people as a white belt will be total bullshit, reality will hit you in your face. You have to try hard to understand every basic technique and know their limits, figure out when does strength work well and when does it not - You have to go through the stiff stage.\nRelaxation should be a sign of mastery, and yet people mistook it as the cause of mastery.\nYour brain is a muscle It is a well-known1 result that human brains are like muscles, they can be trained to become stronger, and they become weaker if lack training. And most great problem solvers tell people that anyone can get better in problem solving by practicing. So it is natural to ask, does relaxation do the same for problem solving? Is the mind of a mature problem solver more relax when solving problems, comparing to that of a naive one? Could relaxation of mind be a sign of mastery of problem solving skills?\nThere are too many times that I think \u0026ldquo;too hard\u0026rdquo; on a problem: I manipulated the mental pictures so hard that I felt my brain was \u0026ldquo;locked\u0026rdquo;.\nAnalogous to muscles, should our mind be relax when we are using the right \u0026ldquo;brain muscles\u0026rdquo; when solving problems? And what are those brain msucles? How could we deliberately practice so that we acquire the skills of using the right muscles faster?\nI would be really interested if others share their experience, and do they feel the same way on how their mind works when solving problems.\nI don\u0026rsquo;t have references to the research papers that discuss these results, but https://www2.cmich.edu/ess/oss/Documents/Prepare%20for%20Success%20d4.pdf could lead one to them.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/thoughts/relaxation-is-the-result-of-mastery-with-thoughts-about-problem-solving/","summary":"Relaxation as a result of mastery, instead of a prerequisites In many sports/ crafts that I have experienced, relaxation is considered by many the prerequisites of mastery:\nIn playing piano, your fingers, wrists, arms and shoulders have to be relax or you will find it extremely hard to press the right keys fluently; In online games that require fast reactions, rookies hands are stiff and they press buttons with unnecessary strength and range of motion, which stops them from pressing right buttons at the right time smoothly; In jiujitsu, being reasonably relax helps you to perform efficient moves, saving your energy and gives you more choices in different situations in which you have to adapt and react quickly.","title":"Relaxation is the result of mastery. With thoughts about problem solving"},{"content":"Counting binary trees I was thinking about of the problem of balancing a binary tree, and my mind stumbled across to the question \u0026ldquo;How many different binary trees with labelled nodes can you make without changing the traversal order?\u0026rdquo;. After figuring out the answer myself I realized that the numbers of such binary trees are just Catalan numebrs (I was not really into combinatorics - now I am).\nFor example, the following has traversal order 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;6-\u0026gt;7\ngraph TB; A((4))--\u003eB((2)) A--\u003eC((6)); B--\u003eE((1)) B--\u003eF((3)) C--\u003eH((5)) C--\u003eI((7)) But the following also has the same traversal order:\ngraph TB; A((4))--\u003eB((3)) A--\u003eC((5)) B--\u003eE((1)) B~~~Hidden1:::hidden E~~~Hidden2:::hidden E--\u003eX((2)) C~~~Hidden3:::hidden C--\u003eI((7)) I--\u003eJ((6)) I~~~Hidden4:::hidden classDef hidden display: none; If we want to maintain the given traversal order while creating with different binary trees, we are essentially counting the number of binary trees that can represent the array $1, 2, 3, \\dots, n$.\nTo construct a binary tree from the given array, there are 2 ways to do so:\nPicking roots in subtrees To create a tree with the array, let\u0026rsquo;s start with picking one of the items as root. Let\u0026rsquo;s say we pick 3 here:\nNow every items before 3 should appears in the left subtree due the way of how traversal order works. Similar for the right subtrees. In order to construct the left/ right subtree, we need to pick the root for them:\nContinue in the same manner, we will get the whole binary tree:\nIt is not hard to see the recursion in this process. After we pick a root, we need to pick the root for the subtrees, which are the same problem with smaller number of nodes. The sizes of the subtrees depends on the choice of the root. By running through all choices of root we have:\nLet $f(n)$ be the number of binary trees desired.\nThen $f(n) = \\sum\\limits_{i=0}^{n-1} f(i)f(n-1-i)$\nPicking legs Draw $n$ nodes with their 2 \u0026ldquo;legs\u0026rdquo;. There are totally $2n$ \u0026ldquo;legs\u0026rdquo;. Each node has a properties called \u0026ldquo;has_parent\u0026rdquo; to keep track of the existence of parent for each node. Initally they are all false. We perform the following:\nDefine the following operations of a node: a. Pick a leg: the leg will be occupied and we have to attach a node to this leg. The node being attached cannot be the root and cannot have a parent. Set the \u0026ldquo;has_parent\u0026rdquo; to true for the attached node. Note that the choice of node being attached is not relevant as the nodes are unlabelled. b. Nullify a leg: the leg will be occupied but we don\u0026rsquo;t attach a node to it. Pick a node and mark it as the root. Set the \u0026ldquo;has_parent\u0026rdquo; to true. Pick one leg of it. Pick a node which has at least one leg not occipied and either pick a leg or nullify a leg of it. Do step 3 until we have picked $n-1$ legs in total. By this process we enumerate all possible binary trees, except some duplicates: In step 2 we picked a node as the root and force it to have at least 1 leg picked, however the nodes should indifferent and the root picking process broke this. As there are $n$ way to pick a root, we should divide the number of possible binary trees by $n$ for deduplication.\nTo see the number of binary trees generated by this process, note that there are $2n$ legs to be picked, and we pick $n-1$ of them. Hence there are $\\binom{2n}{n-1}$ of them. Dividing it by $n$ gives us $\\frac{(2n)!}{(n)!(n+1)!}$ which is the closed form of Catalan numbers.\nFinally note that the number of distinct unlabelled binary trees is the same as distinct labelled binary trees with a given traversal order - there is only 1 way to fill in labels into the nodes such that they match the given traversal order.\n","permalink":"http://localhost:1313/blogs/mathematics/reinventing-catalan-numbers/","summary":"Counting binary trees I was thinking about of the problem of balancing a binary tree, and my mind stumbled across to the question \u0026ldquo;How many different binary trees with labelled nodes can you make without changing the traversal order?\u0026rdquo;. After figuring out the answer myself I realized that the numbers of such binary trees are just Catalan numebrs (I was not really into combinatorics - now I am).\nFor example, the following has traversal order 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;6-\u0026gt;7","title":"Reinventing Catalan numbers"},{"content":"Defining the machine by describing it In Sipser\u0026rsquo;s Introduction to the theory of computation, a alternative way of defining a Turing machine (other than defining the formal 7-tuple, which is a PITA) - by its \u0026ldquo;description\u0026rdquo;. Example (from the proof of $A_{TM}$ is deciable):\nM = “On input \u0026lt;B, w\u0026gt;, where B is a DFA and w is a string: 1. Simulate B on input w. 2. If the simulation ends in an accept state, accept . If it ends in a nonaccepting state, reject .”\nOf cause, we know that the 7-tuple definition is for the sake of formality, if we have to define and manipulate a 7-tuple machine every time we want to do something with Turing machines, that would be insanely tedious. Hence in the book (and in many other occasions?) the Turing machines are defined by its \u0026ldquo;description\u0026rdquo;, including those relatively complicated ones, such as the Turing machine which can output its own description in the proof of recursion theorem.\nThere is seemingly no restriction of how should a description looks like, so I can create a Turing machine by literally any description, right?\nUnicorn algorithm So what is the problem? In fact this approach makes sense in a great extend: not only it reduces the effort of defining a Turing machine, but it also captures the intuition \u0026ldquo;Turing machines are just programs\u0026rdquo;, so defining a Turing machine is essentially defining an algorithm.\nFor \u0026ldquo;most of the cases\u0026rdquo; this is a safe and good way to deal with Turing machines. But let\u0026rsquo;s consider the following description:\nM = “On input w, give a candy to the unicorn who lives in the Hilbert Grand Hotel. If the unicorn accepts your candy, accept. Otherwise reject .”\nThis is a description right? Does that mean I created a Turing machine and I can call it the unicorn algorithm?\nWell, this is an extreme example, but the point is, the description approach of defining Turing machine is too broad, how can we make sure what we described is not \u0026ldquo;impossible\u0026rdquo;? By \u0026ldquo;possible\u0026rdquo; description, I mean a description that can be computed by a Turing machine, instead of things that cannot be done by Turing machine like ffering a candy to the unicorn. Note that there are some problems that looks pretty sane, but they are beyond the Capability of Turing machines, such as the Busy Beaver problem.\nOK, so what we need is to make sure that what the description is describing can actually be done by a Turing machine, right? Let\u0026rsquo;s call such descriptions as \u0026ldquo;Turing description\u0026rdquo;. But how can we tell? Can we prove/ disprove that? It is also natural to ask: \u0026ldquo;how many Turing description are there?\u0026rdquo;, or the a bit more precise and interesting rephrasing: \u0026ldquo;If I pick a description randomly, how possible is it to be Turing description?\u0026rdquo;\nUnfortunately, we can\u0026rsquo;t.\nFirst of all, by \u0026ldquo;prove/ disprove\u0026rdquo;, it means that we have a step-by-step, mechanical way to deduce its trueness - which means there should be a Turing machine taking the statement as input and return 0 or 1. And this Turing machine has to be a decider if we want to tell whether any given description can be implemented by some Turing machine. Let\u0026rsquo;s call this decider be D and we will show its existence.\nLet\u0026rsquo;s restrict the space of possible description without loss of generality: The description of must be well-defined, as in for any given input, it can tell you whether it accepts or not. (Obviously - otherwise it does not even tell me what does it accept. Note that it is different from assuming it is describing a decider - the description just tells you the result, it does not imply that if you throw an input into the corr. Turing machine (if it exists) the machine will halt.). We also assume takes inputs that Turing machines can take (sure, otherwise we know that is not describing a Turing machine straight away).\nNow note that Turing machine input can be encoded as natural numbers:\nLet the Turing machine be associated with a set of symbols $\\Gamma$ and $|\\Gamma| = N$. Assign numbers $0, 1, 2, \\dots, N-1$ to the symbols. Now the input of tape can be viewed as a N-adic number with the first symbol as the least significant figure. For example when $N = 2$, $0101$ is mapped to $1010_{2} = 10$. Note that this is a bijection between all possible input and $\\mathbb{N}$.\nAlso Note that the existence of blank symbol does not affect our argument, we can simply put it in $\\Gamma$ and consider it as a normal symbol.\nHence a description defined above has implicitly defined a function $f:\\mathbb{N} \\to {0, 1}$ as $0, 1$ represents accept and reject.\nNow, we all know that the set of all possible f is uncountable, and that a given Turing machine can only take countably many distinct input, we can conclude that D does not exist, as it cannot even take all the input that it should take, which is the set of all well-defined description, which is f.\nIs it really\u0026hellip;MOST of the cases? The answer of \u0026ldquo;If I pick a description randomly, how possible is it to be Turing description?\u0026rdquo; should be clear now - \u0026ldquo;almost every function from $\\mathbb{N}$ to ${0, 1}$ is not Turing description, since there are uncountably many functions from $\\mathbb{N}$ to ${0, 1}$, but only countably many Turing machines\u0026rdquo;. This statement trigger a person with mathematics background: \u0026ldquo;So\u0026hellip; does that mean the set of Turing descriptions is of measure zero in the set of functions from $\\mathbb{N}$ to ${0, 1}$\u0026rdquo;. But what is the measure of the set of all functions from $\\mathbb{N}$ to ${0, 1}$?\nI didn\u0026rsquo;t find any articles discussing such measures after a few casual Google searches. We can consider the as a subspace of Baire space and take the \u0026ldquo;common prefix metric\u0026rdquo; i.e. given $x={x_1, x_2, \\dots}$ and $y={y_1, y_2, \\dots}$, $d(x, y) = 0$ if $x=y$, otherwise $d(x, y) = 1/k$ where $k$ is the smallest integer s.t. $x_k \\neq y_k$.\nIn this metric space, the set of Turing descriptions is of (Hausdorff) measure zero: Every Turing description can be covered by an arbitrarily small $k$-ball with it as the center.\nThis looks trivial. Now we want to ask, is non-Turing description dense in Baire space?\nOur intuition tells us that it should true. If you pick a $n$ and flip the value of $f(n)$ of a Turing description, this whole function looks \u0026ldquo;unreasonble\u0026rdquo; now and it would be drastically harder to compute. Now let\u0026rsquo;s check if our Baire space model matches this intuition:\nSuppose the set of non-Turing description is not dense. Then there exists a $f$ in Baire space where $f$ is Turing description s.t. there exists an open ball $B_{1/k}(f)$ s.t. no non-Turing description are in this open ball. But now all element in this open ball are Turing description. Note that there are uncountably many elements in this open ball (since the elements inside are sequences of natural numbers that the first $k$ elements are the same as those of $f$. This whole set is bijective to the whole Baire space and hence uncountable), which means there are uncountably many Turing descriptions in the open ball - which is absurd and we have a contradiction. Hence set of non-Turing description is dense.\nSo, now we can say that Turing descriptions are very rare that almost all functions are not Turing description.\nOutro We have shown that defining a Turing machine by writing down its description can be wrong even when we assume the description is \u0026ldquo;well-formed\u0026rdquo;. As almost every description we write is not Turing description and hence constructing complicated description and just presume that there is a Turing machine for it is problematic.\nTuring descriptions is different from compuatble functions (the Turing machine corr. to a computable function is a decider) and Turing recognizable languages (there could be a Turing machine for a corr. description but it does not halt on every input that the description accepts), but computable functions can be injectively mapped into a subset of the set of Turing machines. Hence we can say that almost every function is uncomputable and the set of uncomputable functions is dense.\nStudy the space of computable functions or other computational objects in topological/ measure theoretical setup would be interesting.\n","permalink":"http://localhost:1313/blogs/computer-science/from-casual-description-of-turing-machine-to-uncompuatble-functions/","summary":"Defining the machine by describing it In Sipser\u0026rsquo;s Introduction to the theory of computation, a alternative way of defining a Turing machine (other than defining the formal 7-tuple, which is a PITA) - by its \u0026ldquo;description\u0026rdquo;. Example (from the proof of $A_{TM}$ is deciable):\nM = “On input \u0026lt;B, w\u0026gt;, where B is a DFA and w is a string: 1. Simulate B on input w. 2. If the simulation ends in an accept state, accept .","title":"From casual description of Turing Machine to the density of uncomputable functions"},{"content":"My pc is overheated how do i cool it down because it is my life1 Someone said that chatGPT will replace Google. Some kids/ early teenagers do often ask chatGPT for things that could have been found by Googling.\nI really hate the idea of replacing search engines by chatGPT. The problem I have with it is not really about AI, but about how do we interact with it.\nWhen we were kids and learnt how to use search engines, we acquired the skills of capturing important parts of the questions in our minds and turn them into keywords. Bashing questions like \u0026ldquo;my hard disk is full how can I get more space\u0026rdquo; usually get worse results than things like \u0026ldquo;hard disk space full solutions\u0026rdquo; as their counterparts.\nSame thing applies to going to library and look for related books in the old days: you can\u0026rsquo;t expect to see there is a book titled almost the same as your question, you have to look for the suitable subject and go to the corr. book shelves, look for keywords and sometimes even look for keywords that is a superset of the things you want and hopefully get the answer in one of the chapters. I speculate that this process is important for training our abstraction ability.\nBut the chatGPT allows people to input their braindump like casually talking and get the answer.\nOf cause, one can still train abstraction ability by something else, like in the older days when there were no search engines, even no internet. But if you know how to search, in most cases it would be faster and you will get more firm answers. And I hate the (bad type of) laziness shown when some people keep bashing long stupid questions into chatGPT and satisfied by shallow answers.\nWe all do this, but in other situations I recently learnt the concepts of complementary cognitive artifacts and competitive cognitive artifacts from David Krakauer: https://nautil.us/will-ai-harm-us-better-to-ask-how-well-reckon-with-our-hybrid-nature-236098/ . You can think of them as a fancier way to say things that help you but make you dumb and those don\u0026rsquo;t. Competitiv cognitive artifacts aren\u0026rsquo;t always bad. Calculators are competitiv cognitive artifacts with respect to the ability of doing arithmetics manually. But do I care being bad at it? No. Is it important in general? Meh, most huamn beings don\u0026rsquo;t need to be good at arithmetics (as long as not bad enough to think 1+1=3 and grab 3 items in a buy-one-get-one-free sale and argue with the cashier). But the fluency of turning a question into something that can be googled reflects a basic level of abstraction as a thinking/ problem solving skill.\nWell, the plot twist is, chatGPT fans can argue: \u0026ldquo;You can throw a question or even an article and ask chatGPT to extract the keywords for you!\u0026rdquo;. But I don\u0026rsquo;t think human beings can outsource thinking to machines.\nFirst, for aesthetic reason, the effort of understanding the world and the insights and wisdom gain by ourselves are what makes our lives worth as human beings. Just getting the answer means nothing.\nMoreover, if the service is provided by someone else, namely big techs at this moment, it is possible that it is not a \u0026ldquo;pure\u0026rdquo; LLM, it can be poisoned by artificial (as in artificial intelligence, lol) regulations. lcamtuf talked about this. Imagine a world in where people all believes the standards and decisions from a inhuman intelligent system which is actually just humans like in Psycho-Pass.\nFinal rant Maybe I am too unreconstructed for this generation, but I think we are entering the age of impatience and chatGPT is just yet another product which encourages the culture of impatience.\nFrom https://www.rebootonline.com/blog/men-bing-from-mars-women-google-from-venus/. Disclaimer: nothing sexism such as suggesting women are doing worse here. Just posting the reference of that imaginery search string which was a classic years ago.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/blogs/thoughts/why-do-i-hate-chatgpt/","summary":"My pc is overheated how do i cool it down because it is my life1 Someone said that chatGPT will replace Google. Some kids/ early teenagers do often ask chatGPT for things that could have been found by Googling.\nI really hate the idea of replacing search engines by chatGPT. The problem I have with it is not really about AI, but about how do we interact with it.\nWhen we were kids and learnt how to use search engines, we acquired the skills of capturing important parts of the questions in our minds and turn them into keywords.","title":"Why do I hate using chatGPT to replace search engines"},{"content":"Introduction Recently I am trying to reverse engineering a simple Windows desktop game made with Unity. I took a look on some reference but found that the setup can be a bit frustrating. This post is attempting to make the setup clearer and more followable. Reference links are listed below.\nTools Our target is to decompile and debug the Assembly-CSharp.dll inside the folder \u0026lt;game root folder\u0026gt;\\\u0026lt;GANE_NAME\u0026gt;_Data\\Managed\\, which contains custom code the game developer wrote, not the code of Unity or other frameworks. It is written in C#. We need the below tools:\ndnSpy dotPeek (patched) de4dot Some files from official Unity Setup If you got a DEBUG build of the game, you are lucky, you can skip almost all the steps below and just use dnSpy to decompile, debug and modify the code (discussed below). But if you are not insider of the game development team and the team is not stupid enough (or, unfortunately, didn\u0026rsquo;t get enough sleep) to release a debug build to production, you will be dealing with a RELEASE build.\nFortunately, there is a way to tweak the release build to debug build.\nStep 1 Check the version of Unity of the game. Open the file \u0026lt;game root folder\u0026gt;\\\u0026lt;game name\u0026gt;_Data\\globalgamemanagers.assets with notepad, the version of Unity will be shown at the beginning.\nStep 2 Go to https://unity.com/releases/editor/archive and download the correct version of Unity. Install it.\nStep 3 Go to \u0026lt;Unity root\u0026gt;\\Editor\\Data\\PlaybackEngines\\windowsstandalonesupport\\Variations\\win32_development_mono if the game is 32bit, otherwise go to \u0026lt;Unity root\u0026gt;\\Editor\\Data\\PlaybackEngines\\windowsstandalonesupport\\Variations\\win64_development_mono.\nCopy Data\\Managed folder to the corr. folder of the game. Copy WindowsPlayer.exe and UnityPlayer.dll. Rename to \u0026lt;GAME_NAME\u0026gt;.exe. Copy \u0026lt;Unity root\u0026gt;\\Editor\\Data\\MonoBleedingEdge\\bin\\mono-2.0-bdwgc.dl to \u0026lt;GAME_NAME\u0026gt;\\MonoBleedingEdge. Create of edit boot.config and ensure the line player-connection-debug=1 presents. Step 4 Download this de4dot fork and compile it locally by Visual Studio.\nStep 5 Run de4dot.exe -f Assembly-CSharp.dll -o \u0026lt;output dll location\u0026gt; -fpdb. Remove the output pdb file.\nStep 6 Load the output dll into dotPeek.\nRight click on the output dll and select Export to Project Check the box Create *.pdb file Export Step 7 Run the following command: \u0026quot;%UNITY_MONO%\\bin\\mono.exe\u0026quot; \u0026quot;%UNITY_MONO%\\lib\\mono\\4.5\\pdb2mdb.exe\u0026quot; \u0026quot;\u0026lt;target_assembly_dll\u0026gt;\u0026quot; where \u0026quot;%UNITY_MONO% is equal to \u0026lt;Unity root\u0026gt;\\Editor\\Data\\MonoBleedingEdge.\nYou will get a .mdb file. Now you can debug with dnSpy the dll located in the same folder.\nReference Debugging Unity Games Let\u0026rsquo;s Play with Fire Wiki DotPeek PDB generation for assemblies without debug directory ","permalink":"http://localhost:1313/blogs/security/unity-game-reversing-1/","summary":"Introduction Recently I am trying to reverse engineering a simple Windows desktop game made with Unity. I took a look on some reference but found that the setup can be a bit frustrating. This post is attempting to make the setup clearer and more followable. Reference links are listed below.\nTools Our target is to decompile and debug the Assembly-CSharp.dll inside the folder \u0026lt;game root folder\u0026gt;\\\u0026lt;GANE_NAME\u0026gt;_Data\\Managed\\, which contains custom code the game developer wrote, not the code of Unity or other frameworks.","title":"Unity Game Reversing(1): Setup"},{"content":"Introduction LOTS project, founded by mrd0x, is a collection of websites which is likely be trusted but can be used to evade detection when conducting phishing, C\u0026amp;C, exfiltration and downloading tools. In this post I will introduce a way to abusing PayPal and hopefully will be contributing to the LOTS project.\nThis series is (intentively) for my ideas on novel exfiltration/ C\u0026amp;C channels.\nExfiltraftion by Paypal In Paypal, one can dispute an order and upload his/ her evidence. This feature can be used as data exfiltration channel.\nWhile the document states that the dispute only accepts JPG, GIF, PNG and PDF, it is not complicated to pass this requirement by prepending the magic number of GIF to the file being uploaded (tested in API sandbox):\nprintf \u0026quot;\\x47\\x49\\x46\\x38\\x37\\x61\u0026quot; | cat - realfile \u0026gt; fakegif\nSucessfully uploaded on Paypal:\nAdvantages Evading SSL inspection\nOrganization which cares employees’ privacy (getting more these years) exempts some websites from SSL inspection:\nHealthcare Payments etc Example: Policy of Geoscience Australia (under Australian gov) ","permalink":"http://localhost:1313/blogs/security/lots-project-paypal/","summary":"Introduction LOTS project, founded by mrd0x, is a collection of websites which is likely be trusted but can be used to evade detection when conducting phishing, C\u0026amp;C, exfiltration and downloading tools. In this post I will introduce a way to abusing PayPal and hopefully will be contributing to the LOTS project.\nThis series is (intentively) for my ideas on novel exfiltration/ C\u0026amp;C channels.\nExfiltraftion by Paypal In Paypal, one can dispute an order and upload his/ her evidence.","title":"LOTS Project - Paypal"},{"content":"Tl;dr - unfixed information disclosure in Prisma Cloud defenders This post is about how to abuse a agent of a cloud security solution to get information which you should not know, like what security controls are applied, what assets the victim owns and the owners of the assets.\nIntroduction A few months ago I was examining the Prisma Cloud configuration of my workplace and accidentally discovered an information disclosure issue of Prisma Cloud defender (the agent). This issue has been reported to Palo Alto as security disclosure, however Palo Alto declared that this is an expected behavior.\nWhile Palo Alto seems not considering it as a problem, I think it is at least a trick for post exploitation. Below is some background knowledge.\nPrisma Cloud Prisma Cloud is a cloud security solution developed by Palo Alto, providing features including monitoring processes in standalone hosts/ run as daemon set in a K8S cluster.\nThis is done by deploying an agent (which is called a defender) in the target host.\nTo do so, one needs permissions to Prisma Cloud in order to get an access token. Defender Manager is the most suitable role for an operator whose manager only wants him to help deploying defenders without letting him/ her know other information, for example monitoring rules.\nDigging the hole host Let\u0026rsquo;s get a Defender Manager role and install a host defender on a linux machine, let\u0026rsquo;s say machine A.\nTake a look at the directory /var/lib/twistlock/local_db/local/\u0026lt;version number\u0026gt;/messages/, connect is a big file containing a lot of configs, while some details are stored in other files in the same directory.\nWhy can I view things that has nothing to do with my host? Other assets like container registries /var/lib/twistlock/local_db/local/\u0026lt;version number\u0026gt;/connect\nFirewall rules that belongs to OTHER hosts /var/lib/twistlock/local_db/local/\u0026lt;version number\u0026gt;/hostAppFirewallPolicy\nContent of custom rules (e.g. process/ network monitoring) /var/lib/twistlock/local_db/local/\u0026lt;version number\u0026gt;/updateCustomRule\nAs a Defender Manager, all you are supposed to have is the root access in the host where the defender to be installed on. However, after installing defenders, you will be able to get information much more than just the info of that host. For example, reading custom rules on other hosts, let say, hosts which are more important and you don\u0026rsquo;t want to get caught when you have the chance to access them.\nThose could help you if you are an internal threat of a company which uses Prisma Cloud but don\u0026rsquo;t know this trick :)\n","permalink":"http://localhost:1313/blogs/security/prisma-cloud-defenders/","summary":"Tl;dr - unfixed information disclosure in Prisma Cloud defenders This post is about how to abuse a agent of a cloud security solution to get information which you should not know, like what security controls are applied, what assets the victim owns and the owners of the assets.\nIntroduction A few months ago I was examining the Prisma Cloud configuration of my workplace and accidentally discovered an information disclosure issue of Prisma Cloud defender (the agent).","title":"Prisma Cloud Defenders"},{"content":"","permalink":"http://localhost:1313/blogs/mathematics/complex-and-harmonic/","summary":"","title":""},{"content":" \u003c!DOCTYPE html\u003e RISC-V Instruction Explainer RISC-V Instruction Explainer Explain Instructions Load Example ","permalink":"http://localhost:1313/riscv-explainer/","summary":" \u003c!DOCTYPE html\u003e RISC-V Instruction Explainer RISC-V Instruction Explainer Explain Instructions Load Example ","title":""}]